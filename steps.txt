Algo / Block diagram for the ppt:
    - Valence / Positivity: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. 
      Tracks with high valence sound more positive
    - https://www.kaggle.com/theoverman/the-spotify-hit-predictor-dataset?select=LICENSE

    - Baseline models: 
        A baseline model is essentially a simple model that acts as a reference in a machine learning project. 
        Its main function is to contextualize the results of trained models. 
        Baseline models usually lack complexity and may have little predictive power.


1] EDA (Exploratory Data Analysis): 
Preprocessing and understanding our data
It's extremely imp to do this since at the end of the day we wanna identify which attributes have more weightage over
a song being a hit / flop. (can't also optimize without knowing which parameters to eliminate)

1. Merge datasets
2. "Describe" the data
3. Check for missing values
4. Check for duplicates and remove them
5. Data Visualization (Figuring out whether there are relations between variables in the datasets)
    a. count -> skewed, even, avg val for an attribute, etc -> Individual fields (1)
    b. Compare a field against another wrt the target (outcome) (2)
6. Data Preparation
    a. points 3 and 4
    b. dropping some variables
    c. Checking for collinearity (exactly what it sounds like -> checking if 2 variables have some sort of a relation
       between them, similar to multiple linear regression val < |0.8|) -> remove one of them

2] ML:
Applying models and loss functions to determine which one's the best
Models used: Logistic Regression, K-Nearest Neighbours (KNN), Random Forest and Gradient Boosting (all are supervised)

1. Creating baseline models for all 4 (using default hyper-parameters as much as possible)
2. Standardize the scale for all the parameters (need it for KNN because it works on distance)
3. Train baseline models
4. Evaluate the models (ROC, AUC, F1, Accuracy)
5. Optimizing each model individually
    a. Logistic Regression 
        - Recursive Feature Elimination (RFE): Optimal no. of features helps in getting a higher significance
        - Run the model again with several 'C' values
        - Test wrt baseline
        Result: Using a higher 'C' and less features
    b. KNN
        - We'll vary with KNN with several 'k' values
        Result: Higher value for k
    c. Random Forest Classifier
        - Recursive Feature Elimination (RFE)
        - Grid search: We give a set of hyperparameters and by brute force it figures what the ideal combination should be
    d. Gradient Boost Classifier
        - Recursive Feature Elimination (RFE)
        - Grid search
6. Choosing the final model
    - Chose Gradient Boost because Random Forest overfitted, because it has a high training but comparitively lower test score
